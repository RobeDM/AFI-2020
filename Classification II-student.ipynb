{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **AFI Escuela de finanzas**\n",
    "\n",
    "![alt text](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTRsvzArVKQ5jGTVEqwdNneQFIgYVvjLPbYNvxAfFV_iktBaf9u&s)\n",
    "\n",
    "## **MÃ¡ster Executive en Data Science y Big Data en Finanzas**\n",
    "\n",
    "**17 de Enero de 2020**\n",
    "\n",
    "# **IoT Use cases**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Practical Session : Classification methods**\n",
    "\n",
    "Starting from a dataset of water consumption that you can find in dataset_eventos.csv.\n",
    "\n",
    "\n",
    "In this lab session we are going to deep in our knowledge about classifiers by managing most well-known classification algorithms. Besides, we are going to review some useful techniques, such as the cross validation process, which will allow us to adjust the free parameters of the classifier. \n",
    "\n",
    "#### ** During this lab we will cover: **\n",
    "\n",
    "#### * Part 1: Linear models*\n",
    "#### * Part 2: K-Nearest Neighbours (K-NN)*\n",
    "#### * Part 3: Support Vector Machines (SVMs) with different kernel funcions*\n",
    "#### * Part 4: Tree based algorithms*\n",
    "#### * Part 5: Neural Networks*\n",
    "\n",
    "\n",
    "As in previous lab session, to implement the different approaches we will base in [Scikit-Learn](http://scikit-learn.org/stable/) python toolbox.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### ** Part 0: Load and prepare the data **\n",
    "\n",
    "Thidataset consists of 6 classes of water consumption (tap, toilet, shower,...)\n",
    "    \n",
    "The next code includes the lines to download this data set and create the training and test data partitions, as well as normalize them.\n",
    "\n",
    "Useful functions: [make_classification( )](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html), [train_test_split( )](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.train_test_split.html) and [StandardScaler( )](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first time that you downlaod the data it can take a while...\n",
      "Dataset size information:\n",
      "n_features: 37\n",
      "n_classes: 6\n",
      "Number of training samples: 5492\n",
      "Number of test samples: 5492\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "np.random.seed(12)\n",
    "\n",
    "def ReadEvents(file):\n",
    "    data = np.loadtxt(file,skiprows=1,delimiter=';',usecols=range(0,37))\n",
    "    labels = np.loadtxt(file,skiprows=1,delimiter=';',usecols=(37,),dtype='str')\n",
    "    (nSamples,nFeatures)=data.shape\n",
    "    randomPermutation = np.random.permutation(nSamples)\n",
    "    data=data[randomPermutation,:]\n",
    "    labels=labels[randomPermutation]\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(np.unique(labels))\n",
    "    labels = le.transform(labels)\n",
    "    return data,labels\n",
    "    \n",
    "\n",
    "###############################################################################\n",
    "# Download the data, if not already on disk and load it as numpy arrays\n",
    "print('The first time that you downlaod the data it can take a while...')\n",
    "import numpy as np\n",
    "#dataset = np.loadtxt('./dataPrepared.csv', delimiter=',',skiprows=1)\n",
    "#X = dataset[:,:-1]\n",
    "#Y=dataset[:,-1]\n",
    "X,Y = ReadEvents('./dataset_eventos.csv')\n",
    "\n",
    "# for machine learning we use the data directly (as relative pixel\n",
    "# positions info is ignored by this model)\n",
    "n_features = X.shape[1]\n",
    "\n",
    "# the label to predict is the id of the person\n",
    "n_classes = np.unique(Y).shape[0]\n",
    "\n",
    "print(\"Dataset size information:\")\n",
    "print(\"n_features: %d\" % n_features)\n",
    "print(\"n_classes: %d\" % n_classes)\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Preparing the data\n",
    "\n",
    "# Initialize the random generator seed to compare results\n",
    "np.random.seed(1)\n",
    "\n",
    "# Split into a training set and a test set using a stratified k fold\n",
    "\n",
    "# split into a training and testing set\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.5)\n",
    "\n",
    "# Normalizing the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Binarize the labels for some feature selection methods\n",
    "set_classes = np.unique(Y)\n",
    "Y_train_bin = label_binarize(Y_train, classes=set_classes)\n",
    "\n",
    "print(\"Number of training samples: %d\" % X_train.shape[0])\n",
    "print(\"Number of test samples: %d\" % X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Part 1: Linear models**\n",
    "\n",
    "Include the necessary code to train and test a classifier based in:\n",
    "1. A logistic regression model: in thiscase adjust the C parameter by CV\n",
    "2. Linear Discrimation Analysis \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test accuracy of LR is 65.33\n",
      "The test accuracy of LDA is 60.67\n"
     ]
    }
   ],
   "source": [
    "###########################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "###########################################################\n",
    "\n",
    "# Logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "rang_C = np.logspace(-3, 3, 10)\n",
    "tuned_parameters = [{'C': rang_C}]\n",
    "nfold = 10\n",
    "\n",
    "# Train a LR model and adjust by CV the parameter C\n",
    "clf_LR  = GridSearchCV(LogisticRegression(),\n",
    "                   tuned_parameters, cv=nfold)\n",
    "clf_LR.fit(X_train, Y_train)# <FILL IN> \n",
    "acc_test_LR=clf_LR.score(X_test,Y_test)# <FILL IN> \n",
    "\n",
    "# LDA \n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "clf_LDA = LDA()\n",
    "clf_LDA.fit(X_train,Y_train)# <FILL IN> \n",
    "acc_test_LDA=clf_LDA.score(X_test,Y_test)# <FILL IN> \n",
    "\n",
    "print(\"The test accuracy of LR is %2.2f\" %(100*acc_test_LR))\n",
    "print(\"The test accuracy of LDA is %2.2f\" %(100*acc_test_LDA))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ** Part 2: K nearest neigbors**\n",
    "\n",
    "A K-NN approach classifies each new data searching its K nearest neighbors (among the training data) and assigning the majority class among these neighbors. As expected, its performance depends on the number of neighbors (K) used.\n",
    "\n",
    "To start to work, let's analyze for different values of K the K-NN performance, both over training and test sets. Use the [KNeighborsClassifier()](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) method to complete the below code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This figure points out the necessity of selecting the adequate value of K. And, as expected, using the training error for such selection would provide a poor generalization.\n",
    "\n",
    "#### ** Selecting the number of neighbors of a K-NN classifier**\n",
    "\n",
    "Therefore, next step will consist of applying a cross validation (CV) process to select the optimum value of K. You can use the [GridSearchCV( )](http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html) function to implement it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV process sucessfully finished\n"
     ]
    }
   ],
   "source": [
    "###########################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "###########################################################\n",
    "\n",
    "from sklearn import neighbors\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Parameters\n",
    "K_max = 50\n",
    "rang_K = np.arange(1, K_max+1)\n",
    "nfold = 10\n",
    "# Define a dictionary with the name of the parameters to explore as a key and the ranges to explores as value\n",
    "tuned_parameters = [{'n_neighbors': rang_K}]\n",
    "\n",
    "\n",
    "# Cross validation proccess \n",
    "clf_base = neighbors.KNeighborsClassifier( )\n",
    "# Define the classfifier with the CV process (use GridSearchCV here!!!)\n",
    "clf =  GridSearchCV(clf_base, tuned_parameters, cv = nfold, scoring = 'accuracy')#<FILL IN>\n",
    "# Train it (this executes the CV)\n",
    "clf.fit(X_train,Y_train)#<FILL IN>\n",
    "\n",
    "print('CV process sucessfully finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the CV process, the classifier object  contains the information of the CV process (next cell explore the parameter \".grid\\_scores\\_\" to obtain this information)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation results:\n",
      "For K = 1, validation accuracy is 56.77 (+/-3.938)%\n",
      "For K = 2, validation accuracy is 62.49 (+/-2.736)%\n",
      "For K = 3, validation accuracy is 62.58 (+/-3.195)%\n",
      "For K = 4, validation accuracy is 63.58 (+/-2.952)%\n",
      "For K = 5, validation accuracy is 62.43 (+/-2.889)%\n",
      "For K = 6, validation accuracy is 64.71 (+/-2.369)%\n",
      "For K = 7, validation accuracy is 64.53 (+/-2.287)%\n",
      "For K = 8, validation accuracy is 63.36 (+/-2.778)%\n",
      "For K = 9, validation accuracy is 65.49 (+/-1.842)%\n",
      "For K = 10, validation accuracy is 64.49 (+/-2.345)%\n",
      "For K = 11, validation accuracy is 64.58 (+/-2.362)%\n",
      "For K = 12, validation accuracy is 65.69 (+/-2.060)%\n",
      "For K = 13, validation accuracy is 65.73 (+/-1.922)%\n",
      "For K = 14, validation accuracy is 65.60 (+/-1.992)%\n",
      "For K = 15, validation accuracy is 65.46 (+/-2.044)%\n",
      "For K = 16, validation accuracy is 66.42 (+/-0.788)%\n",
      "For K = 17, validation accuracy is 65.48 (+/-2.165)%\n",
      "For K = 18, validation accuracy is 66.39 (+/-0.787)%\n",
      "For K = 19, validation accuracy is 66.24 (+/-0.721)%\n",
      "For K = 20, validation accuracy is 66.24 (+/-0.773)%\n",
      "For K = 21, validation accuracy is 66.22 (+/-0.753)%\n",
      "For K = 22, validation accuracy is 66.31 (+/-0.814)%\n",
      "For K = 23, validation accuracy is 66.17 (+/-0.711)%\n",
      "For K = 24, validation accuracy is 65.90 (+/-0.752)%\n",
      "For K = 25, validation accuracy is 65.91 (+/-0.721)%\n",
      "For K = 26, validation accuracy is 65.86 (+/-0.766)%\n",
      "For K = 27, validation accuracy is 65.62 (+/-0.839)%\n",
      "For K = 28, validation accuracy is 65.40 (+/-0.883)%\n",
      "For K = 29, validation accuracy is 65.20 (+/-0.862)%\n",
      "For K = 30, validation accuracy is 65.17 (+/-0.836)%\n",
      "For K = 31, validation accuracy is 65.24 (+/-0.739)%\n",
      "For K = 32, validation accuracy is 65.35 (+/-0.744)%\n",
      "For K = 33, validation accuracy is 65.11 (+/-0.788)%\n",
      "For K = 34, validation accuracy is 65.17 (+/-0.851)%\n",
      "For K = 35, validation accuracy is 65.04 (+/-0.901)%\n",
      "For K = 36, validation accuracy is 65.04 (+/-0.880)%\n",
      "For K = 37, validation accuracy is 65.02 (+/-0.855)%\n",
      "For K = 38, validation accuracy is 64.97 (+/-0.849)%\n",
      "For K = 39, validation accuracy is 65.06 (+/-0.867)%\n",
      "For K = 40, validation accuracy is 65.13 (+/-0.906)%\n",
      "For K = 41, validation accuracy is 64.88 (+/-0.902)%\n",
      "For K = 42, validation accuracy is 64.77 (+/-0.878)%\n",
      "For K = 43, validation accuracy is 64.73 (+/-0.880)%\n",
      "For K = 44, validation accuracy is 64.80 (+/-0.894)%\n",
      "For K = 45, validation accuracy is 64.75 (+/-0.994)%\n",
      "For K = 46, validation accuracy is 64.84 (+/-0.942)%\n",
      "For K = 47, validation accuracy is 64.79 (+/-0.967)%\n",
      "For K = 48, validation accuracy is 64.77 (+/-1.027)%\n",
      "For K = 49, validation accuracy is 64.82 (+/-0.982)%\n",
      "For K = 50, validation accuracy is 64.71 (+/-1.027)%\n"
     ]
    }
   ],
   "source": [
    "# Printing results\n",
    "print(\"Cross validation results:\")\n",
    "\n",
    "paramsFolds = clf.cv_results_['params']\n",
    "meanScoreFolds = clf.cv_results_['mean_test_score']\n",
    "stdScoreFolds = clf.cv_results_['std_test_score']\n",
    "\n",
    "for fold in range(K_max):\n",
    "    params = paramsFolds[fold]\n",
    "    mean_score = meanScoreFolds[fold]\n",
    "    std_score = stdScoreFolds[fold]\n",
    "    print(\"For K = %d, validation accuracy is %2.2f (+/-%1.3f)%%\" \n",
    "          % (params['n_neighbors'], 100*mean_score, 100*std_score / 2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the fields \".best\\_estimator\\_\" and \".best\\_params\\_\" of the classifier generated by the CV process:\n",
    "* \".best\\_estimator\\_\" contains  the final classifier trained with this select value.\n",
    "* \".best\\_params\\_\" is a dictionary with the selected parameters. In our example, \"best\\_params\\_['n\\_neighbors']\" would provide the selected value of K.\n",
    "\n",
    "Save the selected value of K in variable denoted \"K_opt\" and compute the test error of the final classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value optimum of K is 16\n"
     ]
    }
   ],
   "source": [
    "###########################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "###########################################################\n",
    "\n",
    "# Assign to K_opt the value of K selected by CV\n",
    "K_opt = clf.best_params_['n_neighbors']# <FILL IN>\n",
    "print(\"The value optimum of K is %d\" %(K_opt))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you can also compute the test error directly over the classifier object return by the CV process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "query data dimension must match training data dimension",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-23e370352145>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mKNN_acc_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The test accuracy is %2.2f\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mKNN_acc_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    446\u001b[0m                              % self.best_estimator_)\n\u001b[1;32m    447\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscorer_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefit\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultimetric_\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscorer_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/metrics/_scorer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, estimator, X, y_true, sample_weight)\u001b[0m\n\u001b[1;32m    167\u001b[0m                           stacklevel=2)\n\u001b[1;32m    168\u001b[0m         return self._score(partial(_cached_call, None), estimator, X, y_true,\n\u001b[0;32m--> 169\u001b[0;31m                            sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_factory_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/metrics/_scorer.py\u001b[0m in \u001b[0;36m_score\u001b[0;34m(self, method_caller, estimator, X, y_true, sample_weight)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \"\"\"\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod_caller\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"predict\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             return self._sign * self._score_func(y_true, y_pred,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/metrics/_scorer.py\u001b[0m in \u001b[0;36m_cached_call\u001b[0;34m(cache, estimator, method, *args, **kwargs)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;34m\"\"\"Call estimator with method and args and kwargs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcache\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/neighbors/_classification.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0mneigh_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneigh_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0m_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/neighbors/_base.py\u001b[0m in \u001b[0;36mkneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    661\u001b[0m                 delayed_query(\n\u001b[1;32m    662\u001b[0m                     self._tree, X[s], n_neighbors, return_distance)\n\u001b[0;32m--> 663\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgen_even_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    664\u001b[0m             )\n\u001b[1;32m    665\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1002\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1003\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1005\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    833\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    836\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/neighbors/_base.py\u001b[0m in \u001b[0;36m_tree_query_parallel_helper\u001b[0;34m(tree, *args, **kwargs)\u001b[0m\n\u001b[1;32m    488\u001b[0m     \u001b[0munder\u001b[0m \u001b[0mPyPy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m     \"\"\"\n\u001b[0;32m--> 490\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32msklearn/neighbors/_binary_tree.pxi\u001b[0m in \u001b[0;36msklearn.neighbors._kd_tree.BinaryTree.query\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: query data dimension must match training data dimension"
     ]
    }
   ],
   "source": [
    "KNN_acc_test = clf.score(X_test, Y_test)\n",
    "print(\"The test accuracy is %2.2f\" %(100*KNN_acc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Part 3: SVM**\n",
    "\n",
    "SVM is one of the most well-known classifiers due to its good generalization properties in many different applications. Besides, by means of the kernel trick, its linear formulation can easily extended to a non linear fashion. \n",
    "\n",
    "Here, we will test its performance when different kernel functions are used. For this purpose, we can use the [SCV( )](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) method, which let you select the kernel function to be used, and the method GridSearchCV( ) to adjust the different free parameters (C and kernel parameter). \n",
    "\n",
    "Complete the following cells, when it is required, to train in each case a linear SVM (defining kernel='linear' in the method SCV( )), an SVM with gaussian kernel (kernel='rbf') and an SVM with polynomial kernel (kernel='poly'). \n",
    "\n",
    "For each method, adjust the corresponding free parameters with a 10 fold CV process (the ranges to explore are defined at the beginning of each cell). Return the values of selected parameters and the accuracy of the final SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** SVM with gaussian kernel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The C value selected is 46.41588833612773\n",
      "The gamma value selected is 10\n",
      "The test accuracy of the RBF SVM is 66.35\n"
     ]
    }
   ],
   "source": [
    "###########################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "###########################################################\n",
    "from sklearn import svm\n",
    "n_dim=X_train.shape[1]\n",
    "rang_g=np.array([10, 100])\n",
    "rang_gamma=np.array([10, 100])\n",
    "tuned_parameters = [{'C': rang_C, 'gamma': rang_g}]\n",
    "\n",
    "# Train an SVM with gaussian kernel and adjust by CV the parameter C\n",
    "clf_base = svm.SVC(kernel='rbf')\n",
    "selection = np.array([2,9,0,6,4,10,1,7,3])\n",
    "rbf_svc  =  GridSearchCV(clf_base, tuned_parameters, cv = nfold, scoring = 'accuracy')# <FILL IN> \n",
    "rbf_svc.fit(X_train[:,selection],Y_train) # <FILL IN> \n",
    "# Save the values of C and gamma selected and compute the final accuracy\n",
    "C_opt = rbf_svc.best_params_['C']# <FILL IN> \n",
    "g_opt = rbf_svc.best_params_['gamma']# <FILL IN> \n",
    "\n",
    "\n",
    "print(\"The C value selected is \" + str(C_opt))\n",
    "print(\"The gamma value selected is \" + str(g_opt))\n",
    "acc_rbf_svc = rbf_svc.score(X_test[:,selection], Y_test)\n",
    "print(\"The test accuracy of the RBF SVM is %2.2f\" %(100*acc_rbf_svc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** 2.3. SVM with polynomial kernel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "###########################################################\n",
    "\n",
    "rang_d=np.arange(1,5)\n",
    "tuned_parameters = [{'C': rang_C, 'degree': rang_d}]\n",
    "\n",
    "# Train an SVM with polynomial kernel and adjust by CV the parameter C\n",
    "clf_base =  svm.SVC(kernel='poly')\n",
    "poly_svc  = GridSearchCV(clf_base, tuned_parameters, cv = nfold, scoring = 'accuracy')# <FILL IN> \n",
    "poly_svc.fit(X_train,Y_train)# <FILL IN> \n",
    "\n",
    "# Save the values of C and degree selected and compute the final accuracy\n",
    "C_opt = poly_svc.best_params_['C']# <FILL IN> \n",
    "d_opt = poly_svc.best_params_['degree']# <FILL IN> \n",
    "\n",
    "\n",
    "print(\"The C value selected is \" + str(C_opt))\n",
    "print(\"The degree value selected is \" + str(d_opt))\n",
    "acc_poly_svc = poly_svc.score(X_test, Y_test)\n",
    "print(\"The test accuracy of the polynomial SVM is %2.2f\" %(100*acc_poly_svc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Part 4: Trees**\n",
    "\n",
    "** Training a Random Forest**\n",
    "A Random Forest (RF) trains several decision tree classifiers, where each one is trained with different samples and features of the training data, and averages their outputs to improve the final accuracy.\n",
    "\n",
    "Use the RandomForestClassifier( ) function to train a RF classifier and select by cross validation the number of trees to use. The remaining parameters, such as the number of subsampled data or features, can be used with their default values. Return the optimal number of trees to be used and the final accuracy of the RF classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of selected trees is 9\n",
      "The test accuracy of the RF is 80.88\n"
     ]
    }
   ],
   "source": [
    "###########################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "###########################################################\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "rang_n_trees=np.arange(1,10)\n",
    "tuned_parameters = [{'n_estimators': rang_n_trees}]\n",
    "nfold = 10\n",
    "\n",
    "clf_RF  = GridSearchCV(RandomForestClassifier(), tuned_parameters, cv = nfold, scoring = 'accuracy')#<FILL IN>\n",
    "clf_RF.fit(X_train, Y_train)\n",
    "n_trees_opt = clf_RF.best_params_['n_estimators']#<FILL IN>\n",
    "acc_RF = clf_RF.score(X_test,Y_test)#<FILL IN>\n",
    "\n",
    "print(\"The number of selected trees is \" + str(n_trees_opt))\n",
    "print(\"The test accuracy of the RF is %2.2f\" %(100*acc_RF))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Part 5: Neural Networks**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
